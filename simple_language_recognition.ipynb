{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple language recognition\n",
    "\n",
    "The aim of this Notebook is to implement a simple language recognition system. We are going to use a N-Gram system.\n",
    "\n",
    "This exercise is inpired by the first NLP home-work I had to do at the University of : http://tesniere.univ-fcomte.fr/\n",
    "\n",
    "**Step 1:** \n",
    "\n",
    "- choose 3 languages\n",
    "\n",
    "- download 2 texts per language (you can find this on : https://www.gutenberg.org/)\n",
    "\n",
    "My languages : english, french, esperanto.\n",
    "\n",
    "**Step 2:** clean the text files\n",
    "\n",
    "**Step 3:** find the unigrams (warming step)\n",
    "\n",
    "**Step 4:** find the bigrams\n",
    "\n",
    "**Step5:** find if a new text is wether in english, french or esperanto\n",
    "\n",
    "## Clean the text\n",
    "\n",
    "**Warning** : please pay attention to punctuation, upper case...\n",
    "\n",
    "Note 1 : there are many ways to clean a document, for example you can use a regular expression to extract only the words of the text (re.findall(r'\\w+', text), or use specialized libraries such as NLTK.\n",
    "\n",
    "Note 2 : sometimes punctuation can help to detect a language : for instance in spanish with '¿'. Also the alphabet family can tell what language is to be detected (check the differences between latin alphabet and cyrrillic alphabet).\n",
    "\n",
    "**In this Notebook we detect a language based on words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # files place\n",
    "import re # remove punctuation (you can also use string or NLTK)\n",
    "from collections import Counter # build the N-gram frequencies\n",
    "\n",
    "# 1 : clean the text\n",
    "# Inputs : directory path, text list in a specific language\n",
    "# Output : text (string) without punctuation and extra whitespaces\n",
    "def clean_text(directory, file_list):    \n",
    "    text = ''\n",
    "    \n",
    "    # read the file\n",
    "    for file in file_list :\n",
    "        with open(os.path.join(directory, file)) as f:\n",
    "            lines = f.read()\n",
    "            \n",
    "            # punctuation, case and whitespaces\n",
    "            lines = re.sub(r'[^\\w\\s]',' ',lines) \n",
    "            lines = re.sub('\\n', ' ', lines)\n",
    "            lines = lines.lower()\n",
    "            lines = re.sub(' +', ' ', lines)\n",
    "            \n",
    "            text += lines\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text for esperanto :  estis iam dudek kvin stansoldatoj ili ciuj estis fratoj car ili devenis de malnova stankulero la paf\n"
     ]
    }
   ],
   "source": [
    "# 2 : apply your clean function on texts\n",
    "\n",
    "# dir and texts\n",
    "directory = 'data/'\n",
    "texts = os.listdir(directory)\n",
    "\n",
    "# french texts :\n",
    "french = [ f for f in texts if 'fr' in f]\n",
    "\n",
    "# esperanto texts :\n",
    "esperanto = [ f for f in texts if 'esp' in f]\n",
    "\n",
    "# english texts :\n",
    "english = [ f for f in texts if 'en' in f]\n",
    "\n",
    "# clean text :\n",
    "clean_fr = clean_text(directory, french)\n",
    "clean_esp = clean_text(directory, esperanto)\n",
    "clean_en = clean_text(directory, english)\n",
    "\n",
    "print(\"Clean text for esperanto : \", clean_esp[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams\n",
    "\n",
    "Find the list of unigrams in the texts and sort them by frequencies.\n",
    "\n",
    "Here we simply use a Counter which counts the occurrences in the text. It's exaclty the same as building a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4895), ('of', 2784), ('and', 2476), ('a', 2104), ('to', 2051), ('in', 1538), ('he', 1349), ('i', 1188), ('was', 1114), ('that', 1057)]\n",
      "\n",
      "There are 5381 unigrams in esperanto texts.\n",
      "\n",
      "There are 10180 unigrams in french texts.\n",
      "\n",
      "There are 11197 unigrams in english texts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the unigram frequencies of a text\n",
    "# Input : a text (str)\n",
    "# Output : Counter object with sorted unigrams\n",
    "def unigrams(text):\n",
    "    \n",
    "    text = text.split()\n",
    "    unigrams = Counter(text)\n",
    "    \n",
    "    return unigrams\n",
    "\n",
    "en_unig = unigrams(clean_en)\n",
    "esp_unig = unigrams(clean_esp)\n",
    "fr_unig = unigrams(clean_fr)\n",
    "\n",
    "# some operations you can make on Counter objects :\n",
    "print(en_unig.most_common(10))\n",
    "print('\\nThere are {} unigrams in esperanto texts.\\n'.format(len(esp_unig)))\n",
    "print('There are {} unigrams in french texts.\\n'.format(len(fr_unig)))\n",
    "print('There are {} unigrams in english texts.\\n'.format(len(en_unig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "Build the bigram lists for each text and sort them by frequence.\n",
    "\n",
    "Here we are trying to give an example to lower the data by using the lemmas of words (for french only to give you an example).\n",
    "\n",
    "**warning** : we don't want duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load((\"fr_core_news_md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard text : us l énorme platane qui la couvre l abrite et l ombrage tout entière j aime ce pays et j aime y vivr\n",
      "Lemmatized text : n sou l énorme platane qui le couvre l abriter et l ombrage tout entier j aimer ce pays et j aimer y\n"
     ]
    }
   ],
   "source": [
    "text = nlp(clean_fr)\n",
    "\n",
    "fr_text_lemma = ''\n",
    "\n",
    "for word in text:\n",
    "    fr_text_lemma += word.lemma_ +' '\n",
    "    \n",
    "print('Standard text :', clean_fr[100:200])\n",
    "print('Lemmatized text :', fr_text_lemma[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10180 unigrams in french texts.\n",
      "\n",
      "There are 6415 tokens in the french text lemmatized\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print('There are {} unigrams in french texts.\\n'.format(len(fr_unig)))\n",
    "print('There are {} tokens in the french text lemmatized'.format(len(unigrams(fr_text_lemma))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de la', 292), ('qu il', 256), ('c est', 212), ('de l', 183), ('d un', 174), ('d une', 166), ('je ne', 154), ('à la', 153), ('j ai', 152), ('et il', 137)]\n"
     ]
    }
   ],
   "source": [
    "# Build the bigrams frequencies of the text\n",
    "# Input : text (str)\n",
    "# Output : dictionary of bigram frequencies\n",
    "def bigrams(text):\n",
    "    \n",
    "    bigram_list = []\n",
    "    text = text.split()\n",
    "    \n",
    "    for i in range(len(text) - 2 + 1):\n",
    "        # sliding window starting at position i and containing 2 words\n",
    "        bigram = ' '.join(text[i : i + 2])        \n",
    "        bigram_list.append(bigram)\n",
    "    \n",
    "    bigram_count = Counter(bigram_list)\n",
    "        \n",
    "    return bigram_count\n",
    "\n",
    "french_bg = bigrams(clean_fr)\n",
    "print(french_bg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of the', 680), ('in the', 471), ('it was', 240), ('to the', 213), ('to be', 189), ('on the', 172), ('he had', 170), ('he was', 170), ('at the', 160), ('of a', 156)]\n",
      "[('en la', 319), ('de la', 219), ('sur la', 165), ('kaj la', 113), ('tio ci', 112), ('tiu ci', 98), ('la doktoro', 95), ('al la', 93), ('diris la', 91), ('al si', 85)]\n"
     ]
    }
   ],
   "source": [
    "# rest of the bigrams :\n",
    "english_bg = bigrams(clean_en)\n",
    "print(english_bg.most_common(10))\n",
    "\n",
    "esperanto_bg = bigrams(clean_esp)\n",
    "print(esperanto_bg.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see esperanto and french share common bigrams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between lists\n",
    "\n",
    "In this step we attribute weigths to each words in an ordered list.\n",
    "\n",
    "During the previous steps we created Counter objects to order words by frequencies. Now we use this order\n",
    "to attribute weights to words. The first word of the smaller list will have a weight of the length of this list. Weights decrease at each word in the list.\n",
    "\n",
    "After that we apply padding on greater lists (we add zeros).\n",
    "\n",
    "These steps will help to compare a new text with all the weighted lists. The list with the highter score indicates which language the new text is.\n",
    "\n",
    "**Step 1**: convert Counter objects into list.\n",
    "\n",
    "- **remember** : keep word order (they are ordered by frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter objects for each language (unigrams) : \n",
    "french_c_ug = fr_unig\n",
    "english_c_ug = en_unig\n",
    "esperanto_c_ug = esp_unig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['her', 'car', 'she', 'was', 'driving', 'when', 'a', 'dear', 'appeared', 'and', 'stopped']\n"
     ]
    }
   ],
   "source": [
    "# Convert Counter object into a list,\n",
    "# Input : Counter object of a specific language\n",
    "# Output : ordered list respecting the order of the words in this Counter\n",
    "\n",
    "def counter_to_list(counter_object):\n",
    "    \n",
    "    dic = dict(counter_object)\n",
    "    # output : {'she': 1, 'was': 1, 'driving': 1, 'her': 2, 'car': 2, ... }\n",
    "    \n",
    "    # this step is necessary to respect the order of words in the Counter object\n",
    "    dic = sorted(dic.items(), key=lambda x : x[1], reverse= True)\n",
    "    # output : [('her', 2), ('car', 2), ('she', 1), ('was', 1), ('driving', 1), ('when', 1), ...]\n",
    "\n",
    "    ordered_list = [ i[0] for i in dic ]\n",
    "    # output : ['her', 'car', 'she', 'was', 'driving', 'when', ...]\n",
    "    \n",
    "    return ordered_list\n",
    "\n",
    "# test :\n",
    "test_text = \"she was driving her car when a dear appeared and stopped her car\"\n",
    "counter_object = Counter(test_text.split())\n",
    "print(counter_to_list(counter_object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 2768), ('et', 2239), ('la', 2002), ('le', 1675), ('il', 1461), ('à', 1424), ('les', 1344), ('l', 1302), ('un', 1230), ('je', 1200)]\n",
      "['de', 'et', 'la', 'le', 'il', 'à', 'les', 'l', 'un', 'je']\n",
      "[('the', 4895), ('of', 2784), ('and', 2476), ('a', 2104), ('to', 2051), ('in', 1538), ('he', 1349), ('i', 1188), ('was', 1114), ('that', 1057)]\n",
      "['the', 'of', 'and', 'a', 'to', 'in', 'he', 'i', 'was', 'that']\n",
      "[('la', 2586), ('kaj', 1525), ('de', 773), ('mi', 644), ('en', 615), ('li', 569), ('si', 536), ('al', 526), ('ne', 500), ('ci', 408)]\n",
      "['la', 'kaj', 'de', 'mi', 'en', 'li', 'si', 'al', 'ne', 'ci']\n"
     ]
    }
   ],
   "source": [
    "# Convert all Counter objects into ordered lists:\n",
    "french_l_ug = counter_to_list(french_c_ug)\n",
    "english_l_ug = counter_to_list(english_c_ug)\n",
    "esperanto_l_ug = counter_to_list(esperanto_c_ug)\n",
    "\n",
    "# Check if the conversing is right :\n",
    "print(french_c_ug.most_common(10))\n",
    "print(french_l_ug[0:10])\n",
    "print(english_c_ug.most_common(10))\n",
    "print(english_l_ug[0:10])\n",
    "print(esperanto_c_ug.most_common(10))\n",
    "print(esperanto_l_ug[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 :** associate weights to words in the ordered list.\n",
    "\n",
    "The first weight of the first word in the smaller list is equal to the size of this list. Then the weights decrease. Also we apply padding to the larger lists (see example).\n",
    "\n",
    "\n",
    "- L1 = [a, b, c, d, e]\n",
    "- L2 = [a, b, f, c, g, d]\n",
    "- weights of L1 =  [5, 4, 3, 2, 1]\n",
    "- weights of L2 = [5, 4, 3, 2, 1, 0]\n",
    "\n",
    "**Step 3 :** multiply and sum word's weights between the 2 lists \n",
    "\n",
    "- S(L1, L2) = 5x5 +4x4 + 3x2 + 2x0 = 25 + 16 + 6 + 0 = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# admit that l1 is always smaller than l2\n",
    "def similarity_lists(l1, l2):\n",
    "    \n",
    "    w_small_l = []\n",
    "    \n",
    "    for w in range(1, len(l1) +1):\n",
    "        w_small_l.append(w)\n",
    "    # [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # 1st word's weight = size of the list :\n",
    "    w_small_l.reverse()\n",
    "    # [5, 4, 3, 2, 1]\n",
    "    \n",
    "    # padd the large list with zeros :\n",
    "    padding_val = len(l2) - len(l1)\n",
    "    # 1\n",
    "    \n",
    "    # weights of the large list :\n",
    "    w_large_l = w_small_l  + ([0] * padding_val)\n",
    "    # [5, 4, 3, 2, 1, 0]\n",
    "    \n",
    "    # assign weights to words to each list :\n",
    "    dic1 = {word : w for word, w in zip(l1, w_small_l)}\n",
    "    dic2 = {word : w for word, w in zip(l2, w_large_l)}\n",
    "    # {'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1}\n",
    "    # {'a': 5, 'b': 4, 'f': 3, 'c': 2, 'g': 1, 'd': 0}\n",
    "    \n",
    "    # sum the product of weights :\n",
    "    count = 0\n",
    "    for val1, poids1 in dic1.items():\n",
    "        for val2, poids2 in dic2.items():\n",
    "            #print(val1, val2)\n",
    "            if val1 == val2:\n",
    "                count += poids1 * poids2\n",
    "    \n",
    "    return count\n",
    "\n",
    "# test with small lists of char : \n",
    "l1 = ['a', 'b', 'c', 'd', 'e']\n",
    "l2 = ['a', 'b', 'f', 'c', 'g', 'd']\n",
    "print(similarity_lists(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text 'i love to eat icecream' is probably in english\n",
      "I don't understand the sentence 'i love ice' , tell me more\n",
      "Your text 'je vais à la piscine avec mes amis' probably in french\n",
      "Your text 'La cielo nin gardu de vido de sceno komenco oni vidadis en la semitajo' is probably in esperanto\n",
      "Your text 'La cielo nin gardu de vido de sceno' probably in french\n"
     ]
    }
   ],
   "source": [
    "# Check simlarity between a text in an unknown language and our 3 ordered lists of unigrams\n",
    "\n",
    "unk_texts = ['i love to eat icecream', 'i love ice', 'je vais à la piscine avec mes amis', \n",
    "             'La cielo nin gardu de vido de sceno komenco oni vidadis en la semitajo', \n",
    "             'La cielo nin gardu de vido de sceno']\n",
    "\n",
    "for text in unk_texts:\n",
    "    # unigrams of this text (don't forget to clean it if you are using a larger file):\n",
    "    unk_ug = unigrams(text)\n",
    "\n",
    "    sim_english = similarity_lists(unk_ug, english_l_ug)\n",
    "    sim_french = similarity_lists(unk_ug, french_l_ug)\n",
    "    sim_esperanto = similarity_lists(unk_ug, esperanto_l_ug)\n",
    "\n",
    "    # scores\n",
    "    #print(sim_english)\n",
    "    #print(sim_esperanto)\n",
    "    #print(sim_french)\n",
    "    \n",
    "    # Tell to the user which language is detected:\n",
    "\n",
    "    if sim_english > sim_esperanto and sim_english > sim_french:\n",
    "        print('Your text \\'{}\\' is probably in english'.format(text))\n",
    "    elif sim_french > sim_esperanto and sim_french > sim_english:\n",
    "        print('Your text \\'{}\\' probably in french'.format(text))\n",
    "    elif sim_esperanto > sim_french and sim_esperanto > sim_english:\n",
    "        print('Your text \\'{}\\' is probably in esperanto'.format(text))\n",
    "    elif sim_esperanto == 0 or sim_english == 0 or sim_french == 0:\n",
    "        print('I don\\'t understand the sentence \\'{}\\' , tell me more'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that here we are checking sentences\n",
    "\n",
    "See the results, the system can't always give the right answer.\n",
    "\n",
    "Esperanto looks like french a lot, because they have some shared tokens. More precisely esperanto is a melting-pot of a lot of languages in the world, because it aims to be THE international language ! More info here : https://fr.wikipedia.org/wiki/Esp%C3%A9ranto\n",
    "\n",
    "Also you can check the language by using bigrams.\n",
    "\n",
    "You will have more chance to detect the right language with a lot of text ! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
